{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicción de fugas de clientes en una compañía telefónica (3.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una compañía telefónica está interesada en que desarrollemos un modelo que prediga los **100 clientes actuales** (dataset de explotación) que tienen más probabilidad de abandonar la compañía. Para ello nos proporcionan una base de datos **fuga_clientes_empresa_telefonica_construccion.csv** (en carpeta data/) con casos etiquetados, que usaremos para construir nuestro modelo de predicción.\n",
    "\n",
    "Los campos de esta base de datos son:\n",
    "\n",
    "* **Customer ID**\n",
    "\n",
    "* **network_age**: antigüedad del cliente en días\n",
    "\n",
    "* **Customer tenure in months:** antigüedad del cliente en meses\n",
    "\n",
    "* **Total Spend in Months 1 and 2:** gasto total del cliente en los meses de referencia 1 y 2\n",
    "\n",
    "* **Total SMS Spend:** gasto total en SMS\n",
    "\n",
    "* **Total Data Spend:** gasto total en datos/internet\n",
    "\n",
    "* **Total Data Consumption:** consumo total de datos (en KB) durante el período de estudio\n",
    "\n",
    "* **Total Unique Calls:** número total de llamadas únicas\n",
    "\n",
    "* **Total Onnet spend:** gasto total en llamadas a otros usuarios de la misma red de telefonía\n",
    "\n",
    "* **Total Offnet spend:** gasto total en llamadas a otros usuarios de redes de telefonía diferentes\n",
    "\n",
    "* **Total Call centre complaint calls:** número de llamadas de queja al call center\n",
    "\n",
    "* **Network type subscription in Month 1:** suscripción de tipo de red en el mes 1. Esto indica la suscripción de red preferida de un cliente, lo que puede indicar su tipo de dispositivo: servicio 2G o 3G\n",
    "\n",
    "* **Network type subscription in Month 2:** igual que el anterior pero en el mes posterior\n",
    "\n",
    "* **Churn Status**: el valor es 1 si el cliente abandona la compañía telefónica, 0 si permanece en ella\n",
    "\n",
    "* **Most Loved Competitor network in Month 1:** qué otro proveedor de la competencia prefiere el cliente en el mes 1. En realidad es un conjunto de columnas, cada una enfocada en un proveedor particular\n",
    "\n",
    "* **Most Loved Competitor network in Month 2:** qué otro proveedor de la competencia prefiere el cliente en el mes 2. En realidad es un conjunto de columnas, cada una enfocada en un proveedor particular\n",
    "\n",
    "La variable a predecir es **Churn Status**: el valor es 1 si el cliente **abandona** la compañía, 0 si no la abandona.\n",
    "\n",
    "La compañía también nos proporciona otra base de datos, **fuga_clientes_empresa_telefonica_explotacion.csv**, con información sobre clientes de los que no se sabe ahora mismo si van a permanecer o no en la compañía. Por tanto en esta segunda base de datos todos los valores de la columna **Churn Status** son missing values (NaN).\n",
    "\n",
    "La compañía nos pide que proporcionemos los IDs de los 100 clientes de la base de datos de explotación que con mayor probabilidad vayan a abandonar la compañía. Para ello proporcionaremos como entregable a la compañía un archivo csv con una sola columna, **Customer ID**, y 100 filas que contengan los IDs de los clientes seleccionados.\n",
    "\n",
    "El fichero **ejemplo_fichero_predicciones.csv** contiene un ejemplo con el formato solicitado para este archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_construccion = pd.read_csv(\"./data/fuga_clientes_empresa_telefonica_construccion.csv\")\n",
    "datos_explotacion  = pd.read_csv(\"./data/fuga_clientes_empresa_telefonica_explotacion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Customer ID', 'network_age', 'Customer tenure in month',\n",
       "       'Total Spend in Months 1 and 2', 'Total SMS Spend', 'Total Data Spend',\n",
       "       'Total Data Consumption', 'Total Unique Calls', 'Total Onnet spend',\n",
       "       'Total Offnet spend', 'Total Call centre complaint calls',\n",
       "       'Churn Status', 'Most Loved Competitor network in Month 1_0',\n",
       "       'Most Loved Competitor network in Month 1_Mango',\n",
       "       'Most Loved Competitor network in Month 1_PQza',\n",
       "       'Most Loved Competitor network in Month 1_ToCall',\n",
       "       'Most Loved Competitor network in Month 1_Uxaa',\n",
       "       'Most Loved Competitor network in Month 1_Weematel',\n",
       "       'Most Loved Competitor network in Month 1_Zintel',\n",
       "       'Most Loved Competitor network in Month 2_Mango',\n",
       "       'Most Loved Competitor network in Month 2_PQza',\n",
       "       'Most Loved Competitor network in Month 2_ToCall',\n",
       "       'Most Loved Competitor network in Month 2_Uxaa',\n",
       "       'Most Loved Competitor network in Month 2_Weematel',\n",
       "       'Most Loved Competitor network in Month 2_Zintel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_construccion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Customer ID', 'network_age', 'Customer tenure in month',\n",
       "       'Total Spend in Months 1 and 2', 'Total SMS Spend', 'Total Data Spend',\n",
       "       'Total Data Consumption', 'Total Unique Calls', 'Total Onnet spend',\n",
       "       'Total Offnet spend', 'Total Call centre complaint calls',\n",
       "       'Churn Status', 'Most Loved Competitor network in Month 1_0',\n",
       "       'Most Loved Competitor network in Month 1_Mango',\n",
       "       'Most Loved Competitor network in Month 1_PQza',\n",
       "       'Most Loved Competitor network in Month 1_ToCall',\n",
       "       'Most Loved Competitor network in Month 1_Uxaa',\n",
       "       'Most Loved Competitor network in Month 1_Weematel',\n",
       "       'Most Loved Competitor network in Month 1_Zintel',\n",
       "       'Most Loved Competitor network in Month 2_Mango',\n",
       "       'Most Loved Competitor network in Month 2_PQza',\n",
       "       'Most Loved Competitor network in Month 2_ToCall',\n",
       "       'Most Loved Competitor network in Month 2_Uxaa',\n",
       "       'Most Loved Competitor network in Month 2_Weematel',\n",
       "       'Most Loved Competitor network in Month 2_Zintel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_explotacion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeo de que las columnas son exactamente las mismas en los dos ficheros:\n",
    "sum(datos_construccion.columns != datos_explotacion.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la columna a predecir es \"Churn Status\"\n",
    "# si es 1, el cliente se va de la compañía\n",
    "# si es 0, el cliente se queda\n",
    "\n",
    "# Esta columna se sabe en el dataset de construcción (ejemplos de clientes pasados):\n",
    "datos_construccion[\"Churn Status\"].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sin embargo no se sabe en el dataset de explotación (clientes actuales):\n",
    "datos_explotacion[\"Churn Status\"].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posibles pasos a realizar:\n",
    "1. Tratado de los datos construccion\n",
    "2. Churn Status (valor a predecir)\n",
    "3. Red neuronal, busqueda de las capas y neuronas optimas\n",
    "4. entrenamiento de la red y testeo\n",
    "5. prediccion de explotacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionando los datos\n",
    "Para empezar descartaremos el \"Customer ID\" porque no tiene mucha relevancia a nivel estadístico y meterémos los datos en varibles separandolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['network_age', 'Customer tenure in month', 'Total Spend in Months 1 and 2', 'Total SMS Spend', 'Total Data Spend', 'Total Data Consumption', 'Total Unique Calls', 'Total Onnet spend', 'Total Offnet spend', 'Total Call centre complaint calls', 'Most Loved Competitor network in Month 1_0', 'Most Loved Competitor network in Month 1_Mango', 'Most Loved Competitor network in Month 1_PQza', 'Most Loved Competitor network in Month 1_ToCall', 'Most Loved Competitor network in Month 1_Uxaa', 'Most Loved Competitor network in Month 1_Weematel', 'Most Loved Competitor network in Month 1_Zintel', 'Most Loved Competitor network in Month 2_Mango', 'Most Loved Competitor network in Month 2_PQza', 'Most Loved Competitor network in Month 2_ToCall', 'Most Loved Competitor network in Month 2_Uxaa', 'Most Loved Competitor network in Month 2_Weematel', 'Most Loved Competitor network in Month 2_Zintel']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "atrs = list(datos_construccion.columns)\n",
    "\n",
    "atrs.remove(\"Churn Status\")\n",
    "\n",
    "atrs.remove(\"Customer ID\")\n",
    "print(atrs)\n",
    "\n",
    "X_c = datos_construccion[atrs].values\n",
    "y_c = datos_construccion[\"Churn Status\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deben normalizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_c, y_c, train_size=0.8, random_state=0)\n",
    "scaler = MinMaxScaler((0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "select = SelectKBest(k=23)\n",
    "\n",
    "X_train_selected = select.fit_transform(X_train, y_train)\n",
    "X_test_selected = select.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han separado estos valores toca elegir que estimador puede sernos más útil para este conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_check(grid_param, estimator):\n",
    "\n",
    "    k_tests = RepeatedStratifiedKFold(n_splits=10, n_repeats=5) # Repite tests\n",
    "    grid_search = GridSearchCV(estimator(), grid_param, cv=k_tests, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    param_names =  list(grid_param.keys())\n",
    "    print(\"Estimación de grid search:\")\n",
    "    for param_name in param_names: \n",
    "        print(\"- \"+param_name +\" = \", str(grid_search.best_params_[param_name]))\n",
    "    return grid_search.best_params_\n",
    "import random\n",
    "\n",
    "def random_create_n_select_layers(min_neurons, min_layers, max_neurons, max_layers, num_combinations):\n",
    "    '''\n",
    "    Crea aleatoriamente un conjunto entre unas neuronas y capas dadas\n",
    "        (se ha de proporcionar también el número de combinaciones totales).\n",
    "    '''\n",
    "    combinations = [[(j,)*i for j in range(min_neurons, max_neurons+1)] for i in range(min_layers, max_layers+1)]\n",
    "    \n",
    "    each_layer = int(num_combinations/(max_layers-min_layers+1))\n",
    "\n",
    "    final =list()\n",
    "\n",
    "    for i in range(max_layers):\n",
    "            prov_list = combinations[i][: (max_neurons-min_neurons+1)]\n",
    "            random.shuffle(prov_list)\n",
    "            final+=(prov_list[:each_layer])\n",
    "        \n",
    "    return final\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- var_smoothing =  2.143347050754458e-05\n",
      "Precisión en cada una de las particiones:  [0.59550562 0.6741573  0.73033708 0.6741573  0.66292135 0.68539326\n",
      " 0.74157303 0.75280899 0.68181818 0.64772727]\n",
      "Estimación de la precisión por validación cruzada: 0.68 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "smooth_pow = 100\n",
    "grid_param= {\"var_smoothing\": [i**(-i) for i in range(1,smooth_pow)]}\n",
    "best_params = train_n_check(grid_param, GaussianNB)\n",
    "\n",
    "clf = GaussianNB(var_smoothing= best_params['var_smoothing']) \n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=10) # 5-fold cross-validation\n",
    "\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- max_depth =  2\n",
      "Precisión en cada una de las particiones:  [0.78651685 0.79775281 0.7752809  0.75280899 0.74157303 0.7752809\n",
      " 0.78651685 0.7752809  0.79545455 0.73863636]\n",
      "Estimación de la precisión por validación cruzada: 0.77 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "max_max_depth = 100\n",
    "grid_param= {\"max_depth\": [i for i in range(1,max_max_depth)]}\n",
    "best_params = train_n_check(grid_param, DecisionTreeClassifier)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth= best_params['max_depth'])\n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=10) # 5-fold cross-validation\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- n_neighbors =  16\n",
      "Precisión en cada una de las particiones:  [0.65168539 0.66292135 0.68539326 0.71910112 0.64044944 0.66292135\n",
      " 0.74157303 0.76404494 0.65909091 0.57954545]\n",
      "Estimación de la precisión por validación cruzada: 0.68 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "max_knn = 100\n",
    "grid_param= {\"n_neighbors\": [i for i in range(1,max_knn)]}\n",
    "best_params = train_n_check(grid_param, KNeighborsClassifier)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=best_params['n_neighbors']) \n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=10) # 5-fold cross-validation\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- tol =  0.037037037037037035\n",
      "- solver =  lbfgs\n",
      "- max_iter =  1000\n",
      "- penalty =  l2\n",
      "Precisión en cada una de las particiones:  [0.64044944 0.6741573  0.73033708 0.66292135 0.66292135 0.66292135\n",
      " 0.71910112 0.7752809  0.67045455 0.625     ]\n",
      "Estimación de la precisión por validación cruzada: 0.68 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "tol_pow = 10\n",
    "tol_list= [i**(-i) for i in range(1,tol_pow)]\n",
    "\n",
    "grid_param= {\"tol\":tol_list, \"solver\":[\"lbfgs\"], \"max_iter\": [1000], \"penalty\":[\"l2\"]}\n",
    "best_params = train_n_check(grid_param, LogisticRegression)\n",
    "\n",
    "clf = LogisticRegression(penalty=best_params['penalty'],\n",
    "                         tol=best_params['tol'],\n",
    "                         max_iter=best_params['max_iter'],\n",
    "                        solver=best_params[\"solver\"]) \n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=10) # 5-fold cross-validation\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- max_iter =  9000\n",
      "- hidden_layer_sizes =  (40,)\n",
      "- alpha =  0.0\n",
      "Precisión en cada una de las particiones:  [0.64044944 0.6741573  0.73033708 0.6741573  0.66292135 0.6741573\n",
      " 0.71910112 0.7752809  0.67045455 0.625     ]\n",
      "Estimación de la precisión por validación cruzada: 0.68 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_neurons = 5\n",
    "\n",
    "hidden_layer_sizes = [(10*i,) for i in range(1, max_neurons+1)]\n",
    "\n",
    "grid_param= {\"max_iter\": [9000], \"hidden_layer_sizes\":hidden_layer_sizes,\n",
    "             \"alpha\":[0.0]}\n",
    "\n",
    "best_params = train_n_check(grid_param, MLPClassifier)\n",
    "optimal_neurons = best_params[\"hidden_layer_sizes\"]\n",
    "\n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=10) # 5-fold cross-validation\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación de grid search:\n",
      "- max_iter =  9000\n",
      "- hidden_layer_sizes =  (40,)\n",
      "- alpha =  0.0\n",
      "Precisión en cada una de las particiones:  [0.71910112 0.70786517 0.74157303 0.75706215 0.70056497]\n",
      "Estimación de la precisión por validación cruzada: 0.73 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_neurons = optimal_neurons[0] \n",
    "max_neurons = optimal_neurons[0] + 2\n",
    "min_layers = 1\n",
    "max_layers = 5\n",
    "\n",
    "hidden_layer_sizes = random_create_n_select_layers(min_neurons,min_layers,max_neurons,max_layers, 10)\n",
    "\n",
    "grid_param= {\"max_iter\": [9000], \"hidden_layer_sizes\":hidden_layer_sizes,\n",
    "             \"alpha\":[0.0]}\n",
    "\n",
    "best_params = train_n_check(grid_param, MLPClassifier)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                    max_iter=best_params['max_iter'],\n",
    "                    alpha=best_params['alpha']) \n",
    "\n",
    "scores = cross_val_score(clf, X_train_selected, y_train, cv=5) # 5-fold cross-validation\n",
    "\n",
    "print('Precisión en cada una de las particiones: ', scores)\n",
    "print('Estimación de la precisión por validación cruzada: {:.2f} +/- {:.2f}'.format(scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras estas comprobaciones parece que se puede llegar a obtener mejor predictibilidad con los decision trees, aun así se va a realizar tanto con redes neuronales como con decision trees la siguiente prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes neuronales: 0.6734234234234234\n",
      "Arboles decision: 0.7252252252252253\n"
     ]
    }
   ],
   "source": [
    "scores_nn = list()\n",
    "scores_dt = list()\n",
    "\n",
    "for i in range(40):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(40,), max_iter=3000,alpha=0.0)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "    scores_nn.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=2)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "    scores_dt.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Redes neuronales:\", mean(scores_nn))\n",
    "print(\"Arboles decision:\", mean(scores_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver los árboles de decisión predicen mejor este conjubto de datos, seguramente por que no hay datos suficientes como para que la red neuronal de valores lo suficientemente precisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADF1330' 'ADF1331' 'ADF1345' 'ADF1349' 'ADF1363' 'ADF1372' 'ADF1403'\n",
      " 'ADF1404' 'ADF1410' 'ADF1425' 'ADF1429' 'ADF1431' 'ADF1433' 'ADF1438'\n",
      " 'ADF1439' 'ADF1445' 'ADF1450' 'ADF1452' 'ADF1453' 'ADF1456' 'ADF1462'\n",
      " 'ADF1465' 'ADF1477' 'ADF1481' 'ADF1489' 'ADF1502' 'ADF1503' 'ADF1521'\n",
      " 'ADF1554' 'ADF1559' 'ADF1560' 'ADF1561' 'ADF1562' 'ADF1563' 'ADF1564'\n",
      " 'ADF1567' 'ADF1582' 'ADF1590' 'ADF1596' 'ADF1597' 'ADF1598' 'ADF1601'\n",
      " 'ADF1602' 'ADF1603' 'ADF1604' 'ADF1606' 'ADF1607' 'ADF1614' 'ADF1615'\n",
      " 'ADF1616' 'ADF1617' 'ADF1620' 'ADF1623' 'ADF1624' 'ADF1625' 'ADF1632'\n",
      " 'ADF1634' 'ADF1635' 'ADF1638' 'ADF1639' 'ADF1644' 'ADF1650' 'ADF1656'\n",
      " 'ADF1657' 'ADF1663' 'ADF1668' 'ADF1670' 'ADF1677' 'ADF1678' 'ADF1685'\n",
      " 'ADF1695' 'ADF1698' 'ADF1707' 'ADF1708' 'ADF1719' 'ADF1721' 'ADF1725'\n",
      " 'ADF1726' 'ADF1728' 'ADF1729' 'ADF1734' 'ADF1736' 'ADF1754' 'ADF1759'\n",
      " 'ADF1760' 'ADF1762' 'ADF1763' 'ADF1767' 'ADF1772' 'ADF1773' 'ADF1774'\n",
      " 'ADF1803' 'ADF1806' 'ADF1811' 'ADF1815' 'ADF1817' 'ADF1821' 'ADF1841'\n",
      " 'ADF1848' 'ADF1849' 'ADF1855' 'ADF1895' 'ADF1896' 'ADF1901' 'ADF1907'\n",
      " 'ADF1913' 'ADF1955' 'ADF1959' 'ADF1972' 'ADF1975' 'ADF1978' 'ADF1979'\n",
      " 'ADF1985' 'ADF1995' 'ADF0038' 'ADF0042' 'ADF0065' 'ADF0081' 'ADF0098'\n",
      " 'ADF0101' 'ADF0106' 'ADF0127' 'ADF0137' 'ADF0256' 'ADF0262' 'ADF0267'\n",
      " 'ADF0285' 'ADF0291' 'ADF0293' 'ADF0297' 'ADF0304' 'ADF0313' 'ADF0317'\n",
      " 'ADF0320' 'ADF0333' 'ADF0339' 'ADF0351' 'ADF0354' 'ADF0363' 'ADF0371'\n",
      " 'ADF0377' 'ADF0391' 'ADF0409' 'ADF0410' 'ADF0417' 'ADF0420' 'ADF0421'\n",
      " 'ADF0440' 'ADF0442' 'ADF0444' 'ADF0446' 'ADF0447' 'ADF0451' 'ADF0457'\n",
      " 'ADF0458' 'ADF0460' 'ADF0475' 'ADF0480' 'ADF0484' 'ADF0488' 'ADF0490'\n",
      " 'ADF0491' 'ADF0494' 'ADF0511' 'ADF0513' 'ADF0517' 'ADF0524' 'ADF0525'\n",
      " 'ADF0537' 'ADF0539' 'ADF0549' 'ADF0561' 'ADF0580' 'ADF0584' 'ADF0585'\n",
      " 'ADF0590' 'ADF0596' 'ADF0597' 'ADF0601' 'ADF0603' 'ADF0607' 'ADF0621'\n",
      " 'ADF0622' 'ADF1031' 'ADF1045' 'ADF1046' 'ADF1051' 'ADF1060' 'ADF1063'\n",
      " 'ADF1079' 'ADF1088' 'ADF1095' 'ADF1096' 'ADF1105' 'ADF1117' 'ADF1119'\n",
      " 'ADF1123' 'ADF1130' 'ADF1131' 'ADF1137' 'ADF1141' 'ADF1142' 'ADF1147'\n",
      " 'ADF1148' 'ADF1156' 'ADF1161' 'ADF1163' 'ADF1166' 'ADF1171' 'ADF1182'\n",
      " 'ADF1183' 'ADF1193' 'ADF1205' 'ADF1206' 'ADF1213' 'ADF1216' 'ADF1223'\n",
      " 'ADF1231' 'ADF1236' 'ADF1238' 'ADF1243' 'ADF1249' 'ADF1256' 'ADF1260'\n",
      " 'ADF1263' 'ADF1265' 'ADF1270' 'ADF1280' 'ADF1286' 'ADF1303' 'ADF1311'\n",
      " 'ADF1321' 'ADF1343' 'ADF1368' 'ADF1377' 'ADF1384' 'ADF1386' 'ADF1390'\n",
      " 'ADF1391' 'ADF1398' 'ADF1400' 'ADF1401' 'ADF1426' 'ADF1451' 'ADF1454'\n",
      " 'ADF1467' 'ADF1468' 'ADF1472' 'ADF1483' 'ADF1509' 'ADF1512' 'ADF1515'\n",
      " 'ADF1520' 'ADF1555' 'ADF1556' 'ADF1572' 'ADF1579' 'ADF1581' 'ADF1592'\n",
      " 'ADF1593' 'ADF1612' 'ADF1613' 'ADF1629' 'ADF1631' 'ADF1641' 'ADF1701'\n",
      " 'ADF1705' 'ADF1709' 'ADF1723' 'ADF1735' 'ADF1739' 'ADF1740' 'ADF1743'\n",
      " 'ADF1752' 'ADF1764' 'ADF1775' 'ADF1780' 'ADF1823' 'ADF1829' 'ADF1888'\n",
      " 'ADF1892' 'ADF1906' 'ADF1915' 'ADF1954' 'ADF1956' 'ADF1965' 'ADF1967'\n",
      " 'ADF1971' 'ADF1982']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "X_ex = datos_explotacion[atrs].values\n",
    "ids = datos_explotacion['Customer ID'].values\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X_train_selected, y_train)\n",
    "y_ex = clf.predict(X_ex)\n",
    "\n",
    "predicted = list()\n",
    "for i in range (100):\n",
    "    predicted.append(ids[i])\n",
    "\n",
    "\n",
    "print(ids) \n",
    "\n",
    "# a csv\n",
    "df = pd.DataFrame()\n",
    "df['Customer_ID'] = ids\n",
    "df.to_csv(\"file_predictions.csv\", index=False)      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
